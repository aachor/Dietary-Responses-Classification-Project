# Identifying Inappropriate and Unsafe ChatGPT Responses to Dietary Struggles
## Project Overview
This thesis seeks to exploit the first publicly available nutritional counseling dataset to investigate ways of identifying inappropriate/unsafe responses from ChatGPT to dietary struggles. The recently created dataset by Balloccu et al.(2024) includes 2,420 dietary struggles, provided by crowd-workers, along with 96,800 supportive texts. These texts, which are nutritional counseling generated by ChatGPT in response to the struggles, have all been annotated for safety. Due to the innovative use of Human-AI (HAI) collaboration in the creation process of this dataset, it is aptly named HAI-Coaching. This project aims to build safety classifiers for ChatGPT responses to real world dietary struggles.
## Research Questions
* Research Question 1: Can dietary advice from ChatGPT be classified as appropriate/safe or inappropriate/unsafe by a model without considering the corresponding dietary struggles?
* Research Question 2: Can considering the related dietary struggles help models more accurately differentiate between inappropriate/unsafe and appropriate/safe dietary advice from ChatGPT?
## Research Objectives/Methodology
* Employing Traditional Machine Learning Models to Identify Inappropriate Dietary Advice – Examining the effectiveness of selected traditional Machine learning models to classify dietary advice as either safe or unsafe without considering the corresponding struggles.
* Employing LLMs to Identify Inappropriate Dietary Advice – Investigating the ability of closed source LLMs to distinguish between appropriate and inappropriate dietary advice in relation to the corresponding dietary struggle of the subject using prompt engineering.
* Fine-Tuning an LLM to Identify Inappropriate Dietary Advice – Exploring how opensource large language models can differentiate between appropriate and inappropriate dietary advice in relation to the corresponding dietary struggles of individuals, by fine-tuning them on selected dataset.
## Results
* Research Question 1: Can dietary advice from ChatGPT be classified as appropriate/safe or inappropriate/unsafe by a model without considering the corresponding dietary struggles?
- The findings from this work show that it is impracticable to distinguish between safe and unsafe dietary advice from ChatGPT without considering the corresponding struggles. This assertion is based on the over 100 cases of exact matches in statements in the safe and unsafe class. Additionally, even after excluding these exact matches, the challenge of handling thousands of close semantic matches remains. It is also important to note that a subset of the safe and unsafe statements in HAI-Coaching contain distinguishing textual features across the classes. However, the exploration of unsafe statements did not uncover any patterns of words or phrases with categorically negative sentiments. Consequently, responses deemed unsafe for one struggle could be safe in the context of another struggle within the corpus.
