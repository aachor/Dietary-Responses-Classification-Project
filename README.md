# Identifying Inappropriate and Unsafe ChatGPT Responses to Dietary Struggles
## Project Overview
This thesis seeks to exploit the first publicly available nutritional counseling dataset to investigate ways of identifying inappropriate/unsafe responses from ChatGPT to dietary struggles. The recently created dataset by Balloccu et al.(2024) includes 2,420 dietary struggles, provided by crowd-workers, along with 96,800 supportive texts. These texts, which are nutritional counseling generated by ChatGPT in response to the struggles, have all been annotated for safety. Due to the innovative use of Human-AI (HAI) collaboration in the creation process of this dataset, it is aptly named HAI-Coaching. This project aims to build safety classifiers for ChatGPT responses to real world dietary struggles.
## Research Questions
* Research Question 1: Can dietary advice from ChatGPT be classified as appropriate/safe or inappropriate/unsafe by a model without considering the corresponding dietary struggles?
* Research Question 2: Can considering the related dietary struggles help models more accurately differentiate between inappropriate/unsafe and appropriate/safe dietary advice from ChatGPT?
## Research Objectives/Methodology
*Employing Traditional Machine Learning Models to Identify Inappropriate Dietary Advice – Examining the effectiveness of selected traditional Machine learning models to classify dietary advice as either safe or unsafe without considering the corresponding struggles.
*Employing LLMs to Identify Inappropriate Dietary Advice – Investigating the ability of closed source LLMs to distinguish between appropriate and inappropriate dietary advice in relation to the corresponding dietary struggle of the subject.
*Fine-Tuning an LLM to Identify Inappropriate Dietary Advice – Exploring how opensource large language models can differentiate between appropriate and inappropriate dietary advice in relation to the corresponding dietary struggles of individuals, by fine-tuning them on selected dataset.
